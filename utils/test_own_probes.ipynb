{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12aa85624dff3eea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test model performance on own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda97c099897bae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03581d-4496-4c21-b494-50fb9fdae39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "path = \"./gesture_data/\"\n",
    "\n",
    "for root, dirs, files in os.walk(path, topdown=False):\n",
    "    for name in dirs:\n",
    "        old_path = os.path.join(root, name)\n",
    "        new_name = name.replace(\" \", \"_\")\n",
    "        new_path = os.path.join(root, new_name)\n",
    "        if old_path != new_path:\n",
    "            os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c083f5191f197b2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. Load Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc65a31-7ec1-4f01-96e4-8c18fe32cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "JESTER_DATASET_DIR_PATH = os.path.join(\"..\", \"..\", \"datasets\")\n",
    "columns = [\"label\"]\n",
    "labels_info = pd.read_csv(os.path.join(JESTER_DATASET_DIR_PATH, \"info\", \"jester-v1-labels.csv\"), names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ca71658d127d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "own_test_df = pd.read_csv(os.path.join(\".\", \"gesture_data\", \"own_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d031d8187380",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "own_test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e80cc69133085",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Change paths to be compatible with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfb121bfe3a689",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def edit_path(path: str) -> str:\n",
    "    return \"/\".join(path.split(\"/\")[:-1])\n",
    "\n",
    "own_test_df[\"path\"] = own_test_df[\"path\"].apply(lambda path: edit_path(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d510af7aa7ed48",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "own_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36164319cef05dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "own_test_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d324750189fe812",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "own_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0db526-b096-4ea9-ba47-9bad2a52cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jester_labels = {}\n",
    "jester_labels_id = {}\n",
    "with open(os.path.join(JESTER_DATASET_DIR_PATH, \"info\", 'jester-v1-labels.csv')) as f:\n",
    "    for idx2, line in enumerate(f):\n",
    "        jester_labels[line.strip()] = idx2\n",
    "        jester_labels_id[idx2] = line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3f680-b820-4eb0-8106-1055568b2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_test_df[\"label\"] = own_test_df[\"label_id\"].apply(lambda label_id: jester_labels_id.get(label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0363d7-b534-4dde-96fe-e6e989206796",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022e8ca-fc38-44c2-be3e-70a6a3add061",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_test_df[\"path\"] = own_test_df[\"path\"].apply(lambda path: path.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eba5a4-75cb-4327-8e57-2a050b139812",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee0c67-1ed6-4b15-a6aa-8bf0200dd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f867f-87d2-46ba-a444-f90d18a67eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_len(path: str) -> int:\n",
    "    return len([file for file in os.listdir(path) if file.endswith(\".jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7623366-18c2-40bb-bc83-9540647f7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_test_df[\"seq_len\"] = own_test_df[\"path\"].apply(lambda path: get_sequence_len(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3504c1a-55ac-4574-8100-60d0bd3f3f6b",
   "metadata": {},
   "source": [
    "### 3. Plot distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434398596bfdd067",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "own_test_df['label'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Etykiety')\n",
    "plt.ylabel('Ilość próbek')\n",
    "_ = plt.title('Rozkład próbek po przetworzeniu we własnym zbiorze')\n",
    "ax = plt.gca()\n",
    "ax.set_axisbelow(True)\n",
    "plt.grid()\n",
    "plt.savefig(\"dataset_plots/own_dist.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96bb15-1233-4d36-a859-492fed3707eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "own_test_df['seq_len'].hist(bins=5) \n",
    "plt.xlabel('Długość sekwencji')\n",
    "plt.ylabel('Liczba wystąpień')\n",
    "plt.title('Rozkład długości próbek w zbiorze')\n",
    "ax = plt.gca()\n",
    "ax.set_axisbelow(True)\n",
    "plt.grid()\n",
    "plt.savefig(\"dataset_plots/seq_len_distribution.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ddc5e3-bd13-46c7-895d-6199d5cccd95",
   "metadata": {},
   "source": [
    "### 4. Create Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09db336-252c-43da-8b2b-f9cf3f3cae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [i for i in range(27)]\n",
    "CLASSES_PERCENTAGE = [1/len(CLASSES) for _ in range(len(CLASSES))]\n",
    "CLASSES_WITH_PERCENTAGE = {cls: percentage for cls, percentage in zip(CLASSES, CLASSES_PERCENTAGE)}\n",
    "CLASS_MAPPING = {cls: idx for idx, cls in enumerate(CLASSES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7a6c7-988b-4a0f-b397-615b2da8e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_dir(df: pd.DataFrame, seq_len: int, class_mapping: dict[int, int]):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        video_path = row['path']\n",
    "        label_id = row['label_id']\n",
    "        image_paths = generate_image_paths(video_path, seq_len, row)\n",
    "        sequences.append(image_paths)\n",
    "        mapped_label_id = class_mapping[label_id]\n",
    "        labels.append(mapped_label_id)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InputShape:\n",
    "    weight: int\n",
    "    height: int\n",
    "    channels: int\n",
    "\n",
    "    def as_tuple(self) -> tuple[int, int, int]:\n",
    "        return self.weight, self.height, self.channels\n",
    "\n",
    "\n",
    "def generate_image_paths(video_dir_path: str, seq_len: int, row: pd.Series):\n",
    "    image_paths = []\n",
    "    current_frames: int = row[\"seq_len\"]\n",
    "\n",
    "    if current_frames >= seq_len:\n",
    "        mid_point = current_frames // 2\n",
    "        start_point = max(0, mid_point - seq_len // 2)\n",
    "        end_point = start_point + seq_len\n",
    "\n",
    "        for i in range(start_point + 1, end_point + 1):\n",
    "            img_name = f\"frame_{i:03d}.jpg\"\n",
    "            img_path = os.path.join(video_dir_path, img_name)\n",
    "            image_paths.append(img_path)\n",
    "\n",
    "    else:\n",
    "        padding_needed = seq_len - current_frames\n",
    "        left_padding = padding_needed // 2\n",
    "        right_padding = padding_needed - left_padding\n",
    "\n",
    "        most_left_img = f\"frame_001.jpg\"\n",
    "        most_right_img = f\"frame_{current_frames:03d}.jpg\"\n",
    "\n",
    "        for i in range(left_padding):\n",
    "            img_path = os.path.join(video_dir_path, most_left_img)\n",
    "            image_paths.append(img_path)\n",
    "\n",
    "        for i in range(1, current_frames + 1):\n",
    "            img_name = f\"frame_{i:03d}.jpg\"\n",
    "            img_path = os.path.join(video_dir_path, img_name)\n",
    "            image_paths.append(img_path)\n",
    "\n",
    "        for i in range(right_padding):\n",
    "            img_path = os.path.join(video_dir_path, most_right_img)\n",
    "            image_paths.append(img_path)\n",
    "\n",
    "    if len(image_paths) != seq_len:\n",
    "        raise ValueError(f\"Missing images in dir: {video_dir_path}\")\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_frames(frames, label):\n",
    "    # flip = random.choice([True, False])\n",
    "    brightness_delta = random.uniform(-0.2, 0.2)\n",
    "    contrast_factor = random.uniform(0.8, 1.2)\n",
    "    saturation_factor = random.uniform(0.8, 1.2)\n",
    "    hue_delta = random.uniform(-0.02, 0.02)\n",
    "    noise_stddev = 0.05\n",
    "\n",
    "    augmentations = [\n",
    "        # lambda img: tf.image.flip_left_right(img) if flip else img,\n",
    "        lambda img: tf.image.adjust_brightness(img, brightness_delta),\n",
    "        lambda img: tf.image.adjust_contrast(img, contrast_factor),\n",
    "        lambda img: tf.image.adjust_saturation(img, saturation_factor),\n",
    "        lambda img: tf.image.adjust_hue(img, hue_delta),\n",
    "        lambda img: img + tf.random.normal(tf.shape(img), mean=0.0, stddev=noise_stddev)\n",
    "    ]\n",
    "\n",
    "    chosen_augmentations = random.sample(augmentations, 2)\n",
    "\n",
    "    def process_image(img):\n",
    "        for aug in chosen_augmentations:\n",
    "            img = aug(img)\n",
    "        return img\n",
    "\n",
    "    preprocessed_images = tf.map_fn(process_image, frames, fn_output_signature=tf.float32)\n",
    "\n",
    "    return preprocessed_images, label\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_sequence_from_dir(image_paths: tf.Tensor, label: int, inp_shape: tuple[int, int, int]):\n",
    "    def process_image(img_path):\n",
    "        image = tf.io.read_file(img_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=inp_shape[2])\n",
    "        image = tf.image.resize(image, inp_shape[:2])\n",
    "        return image\n",
    "\n",
    "    preprocessed_images = tf.map_fn(process_image, image_paths, fn_output_signature=tf.float32)\n",
    "\n",
    "    return preprocessed_images, label\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def one_hot_encode(path: tf.Tensor, label: tf.Tensor, classes_num: int):\n",
    "    return path, tf.one_hot(label, classes_num, dtype=tf.int32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def remove_one_dimensions(images: tf.Tensor, label: int):\n",
    "    return tf.squeeze(images), label\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def normalize_frames(frames, label):\n",
    "    return (tf.cast(frames, tf.float32) / 255.0), label\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def add_dimension(frames, label):\n",
    "    frames = tf.expand_dims(frames, axis=0) \n",
    "    return frames, label\n",
    "\n",
    "def create_pipeline(df: pd.DataFrame, *, num_classes: int, image_input_shape: InputShape, seq_len: int, batch_size: int, class_mapping: dict[int, int], is_training: bool = False, cache: bool = False, normalize: bool = False) -> tf.data.Dataset:\n",
    "    ds = create_dataset_from_dir(df, seq_len=seq_len, class_mapping=class_mapping) # (list with paths strs)\n",
    "    ds = ds.map(lambda images, label: one_hot_encode(images, label, num_classes), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(lambda path, label: load_sequence_from_dir(path, label, image_input_shape.as_tuple()), num_parallel_calls=tf.data.AUTOTUNE) # (seq_len, width, height, channels)\n",
    "    if is_training:\n",
    "        ds = ds.map(lambda frames, label: preprocess_frames(frames, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if normalize:\n",
    "        ds = ds.map(lambda frames, label: normalize_frames(frames, label), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "\n",
    "class PipelineConfig:\n",
    "    IMAGE_INPUT_SHAPE = InputShape(224, 224, 3)\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    BATCH_SIZE = 1\n",
    "    SEQ_LEN = 34\n",
    "\n",
    "own_test_ds = create_pipeline(\n",
    "    own_test_df,\n",
    "    num_classes=PipelineConfig.NUM_CLASSES,\n",
    "    image_input_shape=PipelineConfig.IMAGE_INPUT_SHAPE,\n",
    "    seq_len=PipelineConfig.SEQ_LEN,\n",
    "    batch_size=PipelineConfig.BATCH_SIZE,\n",
    "    class_mapping=CLASS_MAPPING,\n",
    "    is_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b7d0c-d424-49bf-8b43-ca0ffbcc6deb",
   "metadata": {},
   "source": [
    "### 5. Build model and load weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d5e00-219b-4543-ab6d-db3f14636ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_from_articles(input_shape: tuple[int, int], num_classes: int):\n",
    "    mobilenet_model = keras.applications.MobileNetV3Large(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=PipelineConfig.IMAGE_INPUT_SHAPE.as_tuple(),\n",
    "        include_preprocessing=True,\n",
    "        pooling=\"avg\",\n",
    "    )\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(input_shape))\n",
    "    model.add(keras.layers.TimeDistributed(mobilenet_model))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.add(keras.layers.LSTM(units=256, return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.0001), name=\"LSTM_1\"))\n",
    "    model.add(keras.layers.Dropout(0.2, name=\"LSTM_DROPOUT_1\"))\n",
    "    model.add(keras.layers.LayerNormalization(name=\"LSTM_LNORM_1\"))\n",
    "    model.add(keras.layers.LSTM(units=256, return_sequences=True, kernel_regularizer=keras.regularizers.l2(0.0001), name=\"LSTM_2\"))\n",
    "    model.add(keras.layers.Dropout(0.2, name=\"LSTM_DROPOUT_2\"))\n",
    "    model.add(keras.layers.LayerNormalization(name=\"LSTM_LNORM_2\"))\n",
    "    model.add(keras.layers.LSTM(units=256, return_sequences=False, kernel_regularizer=keras.regularizers.l2(0.0001), name=\"LSTM_3\"))\n",
    "    model.add(keras.layers.Dropout(0.2, name=\"LSTM_DROPOUT_3\"))\n",
    "    model.add(keras.layers.LayerNormalization(name=\"LSTM_LNORM_3\"))\n",
    "    model.add(keras.layers.Dense(units=256, activation=\"tanh\", kernel_regularizer=keras.regularizers.l2(0.001), name=\"DENSE_1\"))\n",
    "    model.add(keras.layers.Dropout(0.1, name=\"DENSE_DROPOUT_1\"))\n",
    "    model.add(keras.layers.LayerNormalization(name=\"DENSE_LNORM_1\"))\n",
    "    model.add(keras.layers.Dense(units=num_classes, name=\"DENSE_OUTPUT\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "        'accuracy',\n",
    "        'precision',\n",
    "        'recall',\n",
    "        keras.metrics.F1Score(average='weighted')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_from_articles_conv3D(input_shape: tuple[int, int, int, int], num_classes: int):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(input_shape))\n",
    "\n",
    "    model.add(keras.layers.Conv3D(32, kernel_size=(3, 3, 3), strides=(1, 1, 1), kernel_regularizer=keras.regularizers.l2(0.0001), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(1, 1, 1), kernel_regularizer=keras.regularizers.l2(0.0001), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.Conv3D(128, kernel_size=(3, 3, 3), strides=(1, 1, 1), kernel_regularizer=keras.regularizers.l2(0.0001), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(keras.layers.Conv3D(256, kernel_size=(3, 3, 3), strides=(1, 1, 1), kernel_regularizer=keras.regularizers.l2(0.0001), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv3D(256, kernel_size=(3, 3, 3), strides=(1, 1, 1), kernel_regularizer=keras.regularizers.l2(0.0001), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv3D(256, kernel_size=(3, 3, 3), strides=(1, 1, 1), kernel_regularizer=keras.regularizers.l2(0.0001), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.GlobalMaxPooling3D())\n",
    "    model.add(keras.layers.RepeatVector(1))\n",
    "    model.add(keras.layers.LSTM(256, kernel_regularizer=keras.regularizers.l2(0.0001), return_sequences=True))\n",
    "    model.add(keras.layers.LSTM(256, kernel_regularizer=keras.regularizers.l2(0.0001)))\n",
    "    model.add(keras.layers.LayerNormalization(name=\"DENSE_LNORM_1\"))\n",
    "    model.add(keras.layers.Dense(units=256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.0001), name=\"DENSE_LAYER\"))\n",
    "    model.add(keras.layers.LayerNormalization(name=\"DENSE_LNORM_2\"))\n",
    "    model.add(keras.layers.Dense(units=num_classes, name=\"DENSE_OUTPUT\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            'precision',\n",
    "            'recall',\n",
    "            keras.metrics.F1Score(average='weighted')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_model = build_model_from_articles(input_shape=(PipelineConfig.SEQ_LEN, 1280), num_classes=len(CLASSES))\n",
    "# lstm_model = build_model_from_articles_conv3D(input_shape=(PipelineConfig.SEQ_LEN, *PipelineConfig.IMAGE_INPUT_SHAPE.as_tuple()), num_classes=len(CLASSES))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a579a1f2e35be62c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e09f07-4376-4e53-90c6-b56797f1afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.load_weights('../MobileNetLSTM/model_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70498b3f-4ea3-44e3-8eac-64f02de7cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lstm_model.evaluate(own_test_ds)\n",
    "\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])\n",
    "print(\"Test Precision:\", results[2])\n",
    "print(\"Test Recall:\", results[3])\n",
    "print(\"Test F1 Score:\", results[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce413a9-127d-4922-9f89-728a09e378c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for images, labels in own_test_ds:\n",
    "    predictions = lstm_model.predict(images)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    true_classes = np.argmax(labels, axis=1) if labels.shape[-1] > 1 else labels.numpy()\n",
    "\n",
    "    y_pred.extend(predicted_classes)\n",
    "    y_true.extend(true_classes)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=range(len(CLASSES)))\n",
    "\n",
    "jester_labels = {}\n",
    "with open(os.path.join(JESTER_DATASET_DIR_PATH, \"info\", 'jester-v1-labels.csv')) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        jester_labels[idx] = line.strip()\n",
    "\n",
    "display_info = [jester_labels.get(cls) for cls in CLASSES]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_info)\n",
    "\n",
    "disp.plot(\n",
    "    cmap=plt.cm.Blues,\n",
    "    xticks_rotation=45,  \n",
    "    # values_format='.2f'\n",
    "    values_format='.0f'\n",
    ")\n",
    "\n",
    "fig = disp.figure_\n",
    "fig.set_figwidth(30)\n",
    "fig.set_figheight(30)\n",
    "\n",
    "plt.xlabel('Przewidziana Etykieta', fontsize=20)\n",
    "plt.ylabel('Prawdziwa Etykieta', fontsize=20)\n",
    "\n",
    "plt.xticks(fontsize=15, rotation=45, ha='right')\n",
    "plt.yticks(fontsize=15, rotation=0, va=\"center\")\n",
    "\n",
    "for text in disp.text_.ravel():\n",
    "    text.set_fontsize(14)\n",
    "\n",
    "plt.savefig('conf_mat_BEST_norm_test_mobilenet.png')\n",
    "print(\"Plots saved as 'conf_BEST_test_norm.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce29d08-eb10-4495-a055-4a8c8a157014",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9044f7365b44874"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
